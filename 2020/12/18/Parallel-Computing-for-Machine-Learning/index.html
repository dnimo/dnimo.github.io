<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <title>
        Masano
    </title>
    
<link rel="stylesheet" href="/libs/highlight/styles/monokai-sublime.css">

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

<body id="bodyx">
    <div class="hd posts">
    <a href="/index.html"><i class="fa fa-reply replay-btn" aria-hidden="true"></i></a>
    <div class="post-title">
        <p>
            Parallel Computing for Machine Learning
        </p>
        <hr>
    </div>
    <div class="post-content">
        <h3 id="why-paralle-computing-for-ml">why paralle computing for ML?</h3>
<ul>
<li>Deep learning models are big: ResNet-50 has 25M parameters.</li>
<li>Big models are trained on big data, e.g., Imagenet has 14M images.</li>
<li>Big models + big data --&gt; Big computation cost</li>
<li>Parallel computing: using multiple processors to make the computation faster(in terms of wall-clock time)</li>
</ul>
<blockquote>
<p>通过并行计算只能减少钟表时间，不能减少CPU时间和GPU时间</p>
</blockquote>
<h3 id="least-squares-regression">Least Squares Regression</h3>
<p>#### Linear Predictor</p>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/image-20201218202134708-8297698.png" alt="image-20201218202134708" /><figcaption aria-hidden="true">image-20201218202134708</figcaption>
</figure>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/image-20201218202804218-8297693.png" alt="image-20201218202804218" /><figcaption aria-hidden="true">image-20201218202804218</figcaption>
</figure>
<h3 id="parallel-gradient-descent-for-least-squares">Parallel Gradient Descent for Least Squares</h3>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/image-20201218203607101-8297691.png" alt="image-20201218203607101" /><figcaption aria-hidden="true">image-20201218203607101</figcaption>
</figure>
<blockquote>
<p>The bottleneck of GD is at computing the gradient</p>
<p>It is expensive if #samples and #parameters are both big</p>
</blockquote>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/image-20201218204316783-8297688.png" alt="image-20201218204316783" /><figcaption aria-hidden="true">image-20201218204316783</figcaption>
</figure>
<p><strong>Aggregate g(w) = g<sub>1</sub> + g<sub>2</sub></strong></p>
<h3 id="communication">Communication</h3>
<ol type="1">
<li>share memeory
<ol type="1">
<li>can't to big computing</li>
</ol></li>
<li>message passing
<ol type="1">
<li>I/O is bottleneck</li>
</ol></li>
</ol>
<figure>
<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glsbpj8p2mj31260oa46g.jpg" alt="image-20201218204704337" /><figcaption aria-hidden="true">image-20201218204704337</figcaption>
</figure>
<h3 id="synchronous-parallel-gradient-descent-using-mapreduce">Synchronous Parallel Gradient Descent using MapReduce</h3>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glsbpbo4ubj30n60pcdkv.jpg" alt="image-20201218205628107" style="zoom:50%;" /></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glsbp6ua0bj30o40ocn3b.jpg" alt="image-20201218205649201" style="zoom:50%;" /></p>
<ol type="1">
<li>Broadcast: server broadcast the up-to-data parameters w<sub>t</sub> to workers</li>
<li>Map: Workers do computation locally</li>
<li>Reduce: Compute the sum: Gradient</li>
<li>Server updates the parameters</li>
</ol>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/image-20201218210137506.png" alt="image-20201218210137506" /><figcaption aria-hidden="true">image-20201218210137506</figcaption>
</figure>
<h3 id="communication-cost">Communication Cost</h3>
<ol type="1">
<li>Communication complexity: How many words are transmitted between server and workers</li>
<li>Latency: How much time it takes for a packet of data to get from one point to another</li>
<li>Bulk Synchronous</li>
</ol>
<p><img src="https://raw.githubusercontent.com/dnimo/img/master/Parallel-Computing-for-Machine-Learning/image-20201218215235240.png" alt="image-20201218215235240" style="zoom:50%;" /></p>
<h3 id="synchronous-parallel-gradient-descent">Synchronous Parallel Gradient Descent</h3>
<ul>
<li>Characters:client-server architecture, message-passing communication, and asynchronous</li>
<li>(Note that MapReduce is bulk synchronous)</li>
<li>Ray, an open-source software system, supports parameter server</li>
</ul>
<h4 id="asynchronous-gradient-descent">Asynchronous Gradient Descent</h4>
<p>The i-th worker repeats:</p>
<ol type="1">
<li>Pull the up-to-date model parameters w from the server</li>
<li>Compute gradient g<sub>i</sub> using its local data and w</li>
<li>Push g<sub>i</sub> to the server</li>
</ol>
<h4 id="the-server-performs">The server performs:</h4>
<ol type="1">
<li>Receive gradient g<sub>i</sub> from a worker</li>
<li>Update the parameters by: ==w &lt;-- w-alpha · g<sub>i</sub>==</li>
</ol>
<h4 id="pro-and-con-of-asynchronous-algorithms">Pro and con of Asynchronous Algorithms</h4>
<ol type="1">
<li>In practice, asynchronous algorithm are faster than the synchronous</li>
<li>In theory, asnchronous algorithms has slower convergence rate</li>
<li>Asynchronous algorithms have restrictions, e.g., a worker cannot be much slower than the others</li>
</ol>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/Parallel-Computing-for-Machine-Learning/image-20201220193448083.png" alt="image-20201220193448083" /><figcaption aria-hidden="true">image-20201220193448083</figcaption>
</figure>
<h3 id="decentralized-network">Decentralized Network</h3>
<ol type="1">
<li>Charaters:peer-to-peer architecture (no central server), message-passing communication, a node communicate with its neighbors</li>
</ol>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/Parallel-Computing-for-Machine-Learning/image-20201220194440806.png" alt="image-20201220194440806" /><figcaption aria-hidden="true">image-20201220194440806</figcaption>
</figure>
<figure>
<img src="https://raw.githubusercontent.com/dnimo/img/master/Parallel-Computing-for-Machine-Learning/image-20201220194522617.png" alt="image-20201220194522617" /><figcaption aria-hidden="true">image-20201220194522617</figcaption>
</figure>
<ol start="2" type="1">
<li>Decentralized GD and SGD are guaranteed to converge, e.g.</li>
<li>Convergence rate depends on how well the nodes are connected
<ol type="1">
<li>if the nodes forms a complete graph, then it has very fast convergence</li>
<li>if the graph is not strongly connected, then it does not converge</li>
</ol></li>
</ol>
<h5 id="reference">Reference</h5>
<p>:package:<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/dnimo/img/master/article/1705.0905.pdf">Lian and others: Can decentralized algorithms outperform centralized algorithms? In NIPS, 2017.</a></p>

    </div>

    
        <hr class="fhr">
        <div id="vcomments"></div>
    
</div>
    <div class="footer" id="footer">
    <p>Copyright © 2020 <a class="flink" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>-<a class="flink" target="_blank" rel="noopener" href="https://github.com/sanjinhub/hexo-theme-geek">Geek</a>.
        <label class="el-switch el-switch-green el-switch-sm" style="vertical-align: sub;">
            <input type="checkbox" name="switch" id="update_style">
            <span class="el-switch-style"></span>
        </label>
<!--         <script type="text/javascript">
        var cnzz_protocol = (("https:" == document.location.protocol) ? "https://" : "http://");
        document.write(unescape("%3Cspan id='cnzz_stat_icon_1278548644'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/stat.php%3Fid%3D1278548644%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
        </script> -->
    </p>
</div>
<input type="hidden" id="web_style" value="black">
<input type="hidden" id="valine_appid" value="MeWCTePGu9T30hF9ddgxhFos-gzGzoHsz">
<input type="hidden" id="valine_appKey" value="xMcQwnxYVyT5UqgWzMOqVBPz">

<script src="/libs/jquery.min.js"></script>


<script src="/libs/highlight/highlight.pack.js"></script>

<script src='//cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>

<script src="/js/js.js"></script>

<style type="text/css">
.v * {
    color: #698fca;
}

.v .vlist .vcard .vhead .vsys {
    color: #3a3e4a;
}

.v .vlist .vcard .vh .vmeta .vat {
    color: #638fd5;
}

.v .vlist .vcard .vhead .vnick {
    color: #6ba1ff;
}

.v a {
    color: #8696b1;
}

.v .vlist .vcard .vhead .vnick:hover {
    color: #669bfc;
}
</style>
</body>

</html>